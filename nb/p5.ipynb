{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1035c4d0-8921-42e3-bf5a-4ecf2668a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casey Reyes & Joaquin Feria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e51bfa4-0f2e-47f4-816b-d50f77d508d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -D dfs.replication=1 -cp -f data/*.csv hdfs://nn:9000/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc753b80-4100-49a4-8cc1-c7599c81c042",
   "metadata": {},
   "source": [
    "# Part 1: Filtering: RDDs, DataFrames, and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a821c756-cbc1-4439-9108-bccd650fa784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/09 06:32:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .master(\"spark://boss:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512M\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"hdfs://nn:9000/user/hive/warehouse\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8544a80-3245-4b51-86dd-062cffb0290b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1: how many banks contain the word \"first\" in their name, ignoring case? Use an RDD to answer.\n",
    "\n",
    "# TODO: modify to treat the first row as a header\n",
    "# TODO: modify to infer the schema\n",
    "banks_df = (spark.read.format(\"csv\")\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .load(\"hdfs://nn:9000/arid2017_to_lei_xref_csv.csv\"))\n",
    "rdd = banks_df.rdd\n",
    "filtered_banks = rdd.filter(lambda x: \"first\" in x[0].lower())\n",
    "filtered_banks.count()\n",
    "\n",
    "# filtered_bank_names = filtered_banks.collect()\n",
    "# for name in filtered_bank_names:\n",
    "#     print(name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b88f2a05-8f26-4bc3-8774-d7f76297f533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2 how many banks contain the word \"first\" in their name, ignoring case? Use a DataFrame to answer.\n",
    "from pyspark.sql.functions import expr, col, lower\n",
    "\n",
    "col(\"respondent_name\")\n",
    "expr(\"respondent_name\")\n",
    "\n",
    "filtered_df = banks_df.filter(lower(expr(\"respondent_name\")).like(\"%first%\"))\n",
    "filtered_pandas_df = filtered_df.select(\"respondent_name\")\n",
    "filtered_pandas_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c0e5f12-e6d1-4487-a5b9-e7f73e2376ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/09 06:33:31 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/09 06:33:31 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/11/09 06:33:35 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/11/09 06:33:35 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.3\n",
      "23/11/09 06:33:36 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "23/11/09 06:33:41 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/11/09 06:33:42 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/11/09 06:33:42 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/11/09 06:33:42 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3 how many banks contain the word \"first\" in their name, ignoring case? Use Spark SQL to answer.\n",
    "\n",
    "banks_df.write.saveAsTable(\"banks\", mode=\"overwrite\")\n",
    "banks_df.createOrReplaceTempView(\"names\")\n",
    "banks_df.withColumnRenamed(\"respondent_name\", \"name\").createOrReplaceTempView(\"names\")\n",
    "filtered_df = spark.sql(\"SELECT * FROM names WHERE LOWER(name) LIKE '%first%'\")\n",
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf366f-32af-4056-a348-f5da028f744b",
   "metadata": {},
   "source": [
    "## Part 2: Hive Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4a9d6-3cfa-4db4-b0ec-7210f063fb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================>                            (1 + 1) / 2]\r"
     ]
    }
   ],
   "source": [
    "loans_df = (spark.read\n",
    "            .format(\"csv\")\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .load(\"hdfs://nn:9000/hdma-wi-2021.csv\"))\n",
    "            # .createOrReplaceTempView(\"codes\"))\n",
    "(loans_df.write.format(\"csv\")\n",
    "            .bucketBy(8, 'county_code')\n",
    "            .mode(\"overwrite\")\n",
    "            .saveAsTable('loans'))\n",
    "\n",
    "# loans_df.printSchema\n",
    "views_list = [\"ethnicity\", \"race\", \"sex\", \"states\", \"counties\", \"tracts\", \"action_taken\", \"denial_reason\", \"loan_type\", \"loan_purpose\", \"preapproval\", \"property_type\"]\n",
    "for view in views_list:\n",
    "    loans_df.createOrReplaceTempView(view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce6eaf-ed70-4209-8f81-b8e591a84f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q4 what tables are in our warehouse?\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "table_list = tables_df.collect()\n",
    "table_dict = {row['tableName']: row['isTemporary'] for row in table_list}\n",
    "table_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae96cd-e15d-48e0-a198-96c6a0a3d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q5 how many loan applications has the bank \"University of Wisconsin Credit Union\" received in 2020 in this dataset?\n",
    "\n",
    "bank_name = \"University of Wisconsin Credit Union\"\n",
    "total_df = banks_df.join(\n",
    "    loans_df,\n",
    "    loans_df[\"lei\"] == banks_df[\"lei_2020\"],\n",
    "    \"inner\"\n",
    ").filter(banks_df[\"respondent_name\"] == bank_name)\n",
    "total_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457f9c9-91da-43b1-babc-21647100f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q6 what does .explain(\"formatted\") tell us about how Spark executes Q5?\n",
    "\n",
    "#1. The table in input[4], denial_reason\n",
    "#2. It Does not involve hash aggregates\n",
    "\n",
    "total_df.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b284782-aa51-4612-ac10-2cc9d25dc2d9",
   "metadata": {},
   "source": [
    "## Part 3: Grouping Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd2366-f2a2-4c6e-9b1f-5913a9d422d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q7 what are the average interest rates for Wells Fargo applications for the ten counties where Wells Fargo receives the most applications?\n",
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "counties_df = (spark.read\n",
    "            .format(\"csv\")\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .load(\"hdfs://nn:9000/counties.csv\"))\n",
    "\n",
    "joined_df = banks_df.join(\n",
    "    loans_df,\n",
    "    loans_df[\"lei\"] == banks_df[\"lei_2020\"],\n",
    "    \"inner\"\n",
    ").filter(banks_df[\"respondent_name\"].like(\"%Wells Fargo%\"))\n",
    "result = (joined_df\n",
    " .join(counties_df, on=loans_df[\"county_code\"] == counties_df[\"STATE\"] * 1000 + counties_df[\"COUNTY\"])\n",
    " .groupBy(\"NAME\")\n",
    " .agg(avg(\"interest_rate\").alias(\"avg_interest_rate\"), count(\"*\").alias(\"application_count\"))\n",
    " .orderBy(\"application_count\", ascending=False)\n",
    " .select(\"NAME\", \"avg_interest_rate\")\n",
    " .limit(10))\n",
    " # .show())\n",
    "\n",
    "result_list = result.rdd.map(lambda row: row.asDict()).collect()\n",
    "result_dict = {entry[\"NAME\"]: entry[\"avg_interest_rate\"] for entry in result_list}\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ddd221-b81c-4eaa-92b2-4235e81434bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counties = list(result_dict.keys())\n",
    "avg_interest_rates = list(result_dict.values())\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.bar(counties, avg_interest_rates, width=0.5)\n",
    "plt.xlabel('name')\n",
    "plt.ylabel('Average Interest Rate')\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d0e54f-5b0b-4c1b-b7d3-7a36e3ab862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q8 when computing a MEAN aggregate per group of loans, under what situation (when) do we require network I/O between the partial_mean and mean operations?\n",
    "\n",
    "(loans_df.groupBy(\"county_code\").agg({\"interest_rate\": \"mean\"}).explain())\n",
    "(loans_df.groupBy(\"lei\").agg({\"interest_rate\": \"mean\"}).explain())\n",
    "\n",
    "# there is an indication of network I/O during the hashpartitioning(lei#101, 200) Exchange operation. This is because a \n",
    "# there is a network shuffle operation to redistribute data based on the hash of the lei column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebfd864-2e1d-4979-914c-aea16e5e432f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
